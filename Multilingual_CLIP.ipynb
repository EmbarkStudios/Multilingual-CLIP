{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Copy of Multilingual_CLIP.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FreddeFrallan/Multilingual-CLIP/blob/main/Multilingual_CLIP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bmfoDXVoAh3"
      },
      "source": [
        "# Multilingual CLIP\n",
        "\n",
        "## Install Packages and Setup\n",
        "Run this to configure everything, this might take some minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5JU97PayTvv"
      },
      "source": [
        "#@title  { display-mode: \"code\" }\n",
        "\n",
        "import subprocess\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
        "print(\"CUDA version:\", CUDA_version)\n",
        "\n",
        "if CUDA_version == \"10.0\":\n",
        "    torch_version_suffix = \"+cu100\"\n",
        "elif CUDA_version == \"10.1\":\n",
        "    torch_version_suffix = \"+cu101\"\n",
        "elif CUDA_version == \"10.2\":\n",
        "    torch_version_suffix = \"\"\n",
        "else:\n",
        "    torch_version_suffix = \"+cu110\"\n",
        "\n",
        "!pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex\n",
        "!pip install ftfy==5.8\n",
        "!pip install transformers\n",
        "\n",
        "import os\n",
        "import random\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "import clip\n",
        "\n",
        "!git clone https://github.com/FreddeFrallan/Multilingual-CLIP\n",
        "%cd Multilingual-CLIP\n",
        "!bash get-weights.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ssQbLEFoksN"
      },
      "source": [
        "### Load The Multilingual Text Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QMUva872Fr7"
      },
      "source": [
        "from src import multilingual_clip\n",
        "\n",
        "text_model = multilingual_clip.load_model('M-BERT-Distil-40')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56aiJ7CspXWd"
      },
      "source": [
        "### Load The Matching CLIP Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FymFpSD48OFB"
      },
      "source": [
        "clip_model, compose = clip.load('RN50x4')\n",
        "\n",
        "input_resolution = clip_model.input_resolution.item()\n",
        "context_length = clip_model.context_length.item()\n",
        "vocab_size = clip_model.vocab_size.item()\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in clip_model.parameters()]):,}\")\n",
        "print(\"Input resolution:\", input_resolution)\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AleQ_9u5r_FN"
      },
      "source": [
        "### Prepare Image Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDvfAOlOruTq"
      },
      "source": [
        "def prepare_images(compose, img_paths, device):\n",
        "  return [preprocess(Image.open(p)).unsqueeze(0).to(device) for p in img_paths]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcl1enIJxKT9"
      },
      "source": [
        "### Read in the Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0bjULNmxPW5"
      },
      "source": [
        "from PIL import Image\n",
        "main_path = '/content/Multilingual-CLIP/Images/'\n",
        "demo_images = {\n",
        "    'Green Apple': 'green apple.jpg',\n",
        "    'Red Apple': 'red apple.jpg',\n",
        "    'Purple Apple': 'purple apple.png',\n",
        "    'Orange Apple': 'Orange Apple.png',\n",
        "    'Happy Person': 'happy person.jpg',\n",
        "    'Sad Person': 'sad.jpg',\n",
        "}\n",
        "\n",
        "import os\n",
        "#for p in demo_images.values():\n",
        "for p in os.listdir(main_path):\n",
        "  print(os.path.isfile(main_path + p))\n",
        "\n",
        "#images = {name: Image.open(main_path + p) for name, p in demo_images.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkysmzvTFs7I"
      },
      "source": [
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "\n",
        "preprocess = Compose([\n",
        "    Resize(input_resolution, interpolation=Image.BICUBIC),\n",
        "    CenterCrop(input_resolution),\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "image_mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).cuda()\n",
        "image_std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8azUTJvBfJU"
      },
      "source": [
        "# images in skimage to use and their textual descriptions\n",
        "descriptions_en = {\n",
        "    \"page\": \"a page of text about segmentation\",\n",
        "    \"chelsea\": \"a facial photo of a tabby cat\",\n",
        "    \"astronaut\": \"a portrait of an astronaut with the American flag\",\n",
        "    \"rocket\": \"a rocket standing on a launchpad\",\n",
        "    \"motorcycle_right\": \"a red motorcycle standing in a garage\",\n",
        "    \"camera\": \"a person looking at a camera on a tripod\",\n",
        "    \"horse\": \"a black-and-white silhouette of a horse\", \n",
        "    \"coffee\": \"a cup of coffee on a saucer\"\n",
        "    }\n",
        "\n",
        "descriptions = {\n",
        "    \"page\": \"en sida med text om segmentering\",\n",
        "    \"chelsea\": \"ett porträttfoto på en randig katt\",\n",
        "    \"astronaut\": \"ett porträtt av en astronaut med den amerikanska flaggan\",\n",
        "    \"rocket\": \"en raket på sin uppskjutningsplats\",\n",
        "    \"motorcycle_right\": \"en röd motorcykel i ett garage\",\n",
        "    \"camera\": \"en person som tittar på en kamera på ett stativ\",\n",
        "    \"horse\": \"en svartvit siluett\", \n",
        "    \"coffee\": \"en kopp kaffe och ett fat\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s96GWDBFFODv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "7dc64e2c-c731-4602-82aa-80503156b992"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "images = []\n",
        "texts = []\n",
        "plt.figure(figsize=(16, 5))\n",
        "\n",
        "for filename in [filename for filename in os.listdir(skimage.data_dir) if filename.endswith(\".png\") or filename.endswith(\".jpg\")]:\n",
        "    name = os.path.splitext(filename)[0]\n",
        "    if name not in descriptions:\n",
        "        continue\n",
        "\n",
        "    image = preprocess(Image.open(os.path.join(skimage.data_dir, filename)).convert(\"RGB\"))\n",
        "    images.append(image)\n",
        "    texts.append(descriptions[name])\n",
        "\n",
        "    plt.subplot(2, 4, len(images))\n",
        "    plt.imshow(image.permute(1, 2, 0))\n",
        "    plt.title(f\"{filename}\\n{descriptions[name]}\")\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-2901168318c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".png\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdescriptions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'skimage' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1152x360 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDaJfWS0FmGY"
      },
      "source": [
        "image_input = torch.tensor(np.stack(images)).cuda()\n",
        "image_input -= image_mean[:, None, None]\n",
        "image_input /= image_std[:, None, None]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYLVMrJqGDCa"
      },
      "source": [
        "texts = [\"Det här är \" + desc for desc in texts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jh4CEXWKGRJD"
      },
      "source": [
        "with torch.no_grad():\n",
        "    image_features = clip_model.encode_image(image_input).float()\n",
        "    text_features = sweclip(texts).float()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjF_zccgGcTF"
      },
      "source": [
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTj8NZkmGjZ2"
      },
      "source": [
        "count = len(descriptions)\n",
        "\n",
        "plt.figure(figsize=(20, 14))\n",
        "plt.imshow(similarity, vmin=0.1, vmax=0.3)\n",
        "# plt.colorbar()\n",
        "plt.yticks(range(count), texts, fontsize=18)\n",
        "plt.xticks([])\n",
        "for i, image in enumerate(images):\n",
        "    plt.imshow(image.permute(1, 2, 0), extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\n",
        "for x in range(similarity.shape[1]):\n",
        "    for y in range(similarity.shape[0]):\n",
        "        plt.text(x, y, f\"{similarity[y, x]:.2f}\", ha=\"center\", va=\"center\", size=12)\n",
        "\n",
        "for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
        "  plt.gca().spines[side].set_visible(False)\n",
        "\n",
        "plt.xlim([-0.5, count - 0.5])\n",
        "plt.ylim([count + 0.5, -2])\n",
        "\n",
        "plt.title(\"Cosine similarity between text and image features\", size=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJ70HG-DN0KP"
      },
      "source": [
        "from torchvision.datasets import CIFAR100\n",
        "\n",
        "cifar100 = CIFAR100(os.path.expanduser(\"~/.cache\"), transform=preprocess, download=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qL8rTV1vOyC3"
      },
      "source": [
        "swe_cifar_classes = ['äpple', 'akvariefisk', 'bebis', 'Björn', 'bäver', 'säng',\n",
        "                     'bi', 'skalbagge', 'cykel', 'flaska', 'skål', 'pojke',\n",
        "                     'bro', 'buss', 'fjäril', 'kamel', 'burk', 'slott', 'larv',\n",
        "                     'nötkreatur', 'stol', 'schimpans', 'klocka', 'moln',\n",
        "                     'kackerlacka', 'soffa', \"krabba\", 'krokodil', 'kopp', \n",
        "                     'dinosaurie', 'delfin', 'elefant', 'plattfisk', 'skog',\n",
        "                     'räv', 'flicka', 'hamster', 'hus', 'känguru', \n",
        "                     'tangentbord', 'lampa', 'gräsklippare', 'leopard', 'lejon',\n",
        "                     'ödla', 'hummer', 'man', 'lönnträd', 'motorcykel', 'fjäll',\n",
        "                     'mus', 'svamp', 'ekträd', 'orange', 'orkide', 'utter',\n",
        "                     'palmträd', 'päron', 'pickup', 'tall', 'enkel', 'tallrik',\n",
        "                     'vallmo', \"piggsvin\", 'opossum', 'kanin', 'tvättbjörn',\n",
        "                     'stråle', 'väg', 'raket', 'reste sig', 'hav', 'säl', 'haj',\n",
        "                     'argbigga', 'skunk', 'skyskrapa', 'snigel', 'orm', 'Spindel',\n",
        "                     'ekorre', 'spårvagn', 'solros', 'Sötpeppar', 'tabell',\n",
        "                     'tank', 'telefon', 'tv', 'tiger', 'traktor', 'tåg', 'öring',\n",
        "                     'tulpan', 'sköldpadda', 'garderob', 'val', 'pilträd', 'Varg',\n",
        "                     'kvinna', 'mask']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN0zIIIAN0mV"
      },
      "source": [
        "#text_descriptions = [f\"This is a photo of a {label}\" for label in cifar100.classes]\n",
        "text_descriptions = [f\"Det här är ett foto på {label}\" for label in swe_cifar_classes]\n",
        "#text_tokens = [[sot_token] + tokenizer.encode(desc) + [eot_token] for desc in text_descriptions]\n",
        "#text_input = torch.zeros(len(text_tokens), model.context_length, dtype=torch.long)\n",
        "\n",
        "#for i, tokens in enumerate(text_tokens):\n",
        "#    text_input[i, :len(tokens)] = torch.tensor(tokens)\n",
        "\n",
        "#text_input = text_input.cuda()\n",
        "#text_input.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHnJHfW5N3El"
      },
      "source": [
        "with torch.no_grad():\n",
        "    #text_features = model.encode_text(text_input).float()\n",
        "    text_features = sweclip(text_descriptions).float().cpu()\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "text_probs = (100.0 * image_features.cpu() @ text_features.T).softmax(dim=-1)\n",
        "top_probs, top_labels = text_probs.cpu().topk(5, dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di_yxHVaOP-u"
      },
      "source": [
        "plt.figure(figsize=(16, 16))\n",
        "\n",
        "for i, image in enumerate(images):\n",
        "    plt.subplot(4, 4, 2 * i + 1)\n",
        "    plt.imshow(image.permute(1, 2, 0))\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(4, 4, 2 * i + 2)\n",
        "    y = np.arange(top_probs.shape[-1])\n",
        "    plt.grid()\n",
        "    plt.barh(y, top_probs[i])\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.gca().set_axisbelow(True)\n",
        "    plt.yticks(y, [swe_cifar_classes[index] for index in top_labels[i].numpy()])\n",
        "    plt.xlabel(\"probability\")\n",
        "\n",
        "plt.subplots_adjust(wspace=0.5)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRsyaTINOlex"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}